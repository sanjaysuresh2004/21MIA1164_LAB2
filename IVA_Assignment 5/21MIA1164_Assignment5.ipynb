{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNzCqBWiyV0y"
      },
      "source": [
        "# Task 1: Motion Estimation and Event Detection in a Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZqQsfK8ayV00",
        "outputId": "472fd3e5-8190-4609-da75-f8ae57345bef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected Events:\n"
          ]
        }
      ],
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "video_path = '/content/video1.webm'\n",
        "output_path = 'output_video.mp4'\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "ret, prev_frame = cap.read()\n",
        "if not ret:\n",
        "    print(\"Error: Could not read the first frame.\")\n",
        "    cap.release()\n",
        "    exit()\n",
        "\n",
        "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "frame_number = 1\n",
        "event_frames = []\n",
        "motion_threshold = 0.02\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    frame_diff = cv2.absdiff(prev_gray, gray)\n",
        "    _, thresh = cv2.threshold(frame_diff, 30, 255, cv2.THRESH_BINARY)\n",
        "    motion_intensity = np.sum(thresh) / (thresh.shape[0] * thresh.shape[1] * 255)\n",
        "\n",
        "    if motion_intensity > motion_threshold:\n",
        "        timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
        "        cv2.putText(frame, f\"Event Detected at {timestamp:.2f}s\",\n",
        "                    (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
        "        event_frames.append((frame_number, timestamp))\n",
        "\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    cv2.drawContours(frame, contours, -1, (0, 255, 0), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "\n",
        "\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "    prev_gray = gray.copy()\n",
        "    frame_number += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Detected Events:\")\n",
        "for frame_num, timestamp in event_frames:\n",
        "    print(f\"Event at frame {frame_num}, timestamp {timestamp:.2f}s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoA2qfHNyV02"
      },
      "source": [
        "# Task 2: Estimating Sentiments of People in a Crowd â€“ Gesture Analysis and Image Categorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rPAp4JE8yV02",
        "outputId": "aabafc7d-0caf-4240-e29e-a986651ae0eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /content/img4.webp\n",
            "  Person 1: Neutral\n",
            "  Person 2: Sad\n",
            "Overall sentiment: Sad\n",
            "\n",
            "Annotated image saved to annotated_groupofpeople.jpg\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "image_path = '/content/img4.webp'\n",
        "output_image_path = 'annotated_groupofpeople.jpg'\n",
        "\n",
        "img = cv2.imread(image_path)\n",
        "if img is None:\n",
        "    print(f\"Error loading image {image_path}\")\n",
        "else:\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
        "    upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
        "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (11, 11))\n",
        "    skin_mask = cv2.dilate(skin_mask, kernel, iterations=2)\n",
        "    skin_mask = cv2.GaussianBlur(skin_mask, (3, 3), 0)\n",
        "\n",
        "    contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    face_regions = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        if area > 500:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            aspect_ratio = w / float(h)\n",
        "            if 0.75 < aspect_ratio < 1.3:\n",
        "                face_regions.append((x, y, w, h))\n",
        "\n",
        "    sentiments = []\n",
        "    annotated_img = img.copy()\n",
        "\n",
        "    for (x, y, w, h) in face_regions:\n",
        "        face_img = img[y:y+h, x:x+w]\n",
        "\n",
        "        gray_face = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
        "        _, thresh = cv2.threshold(gray_face, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "        contours_feat, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "        features = []\n",
        "        for cnt_feat in contours_feat:\n",
        "            area_feat = cv2.contourArea(cnt_feat)\n",
        "            if area_feat > 50:\n",
        "                fx, fy, fw, fh = cv2.boundingRect(cnt_feat)\n",
        "                features.append((fx, fy, fw, fh))\n",
        "                cv2.rectangle(face_img, (fx, fy), (fx+fw, fy+fh), (255, 0, 0), 1)\n",
        "\n",
        "        face_height = face_img.shape[0]\n",
        "        mouth_candidates = []\n",
        "        for (fx, fy, fw, fh) in features:\n",
        "            if fy > face_height / 2:\n",
        "                mouth_candidates.append((fx, fy, fw, fh))\n",
        "        if mouth_candidates:\n",
        "            mouth = max(mouth_candidates, key=lambda rect: rect[2] * rect[3])\n",
        "            mx, my, mw, mh = mouth\n",
        "            mouth_region = face_img[my:my+mh, mx:mx+mw]\n",
        "            gray_mouth = cv2.cvtColor(mouth_region, cv2.COLOR_BGR2GRAY)\n",
        "            edges = cv2.Canny(gray_mouth, 50, 150)\n",
        "            top_half = edges[0:mh//2, :]\n",
        "            bottom_half = edges[mh//2:mh, :]\n",
        "            top_count = cv2.countNonZero(top_half)\n",
        "            bottom_count = cv2.countNonZero(bottom_half)\n",
        "            if bottom_count > top_count:\n",
        "                sentiment = \"Happy\"\n",
        "            elif top_count > bottom_count:\n",
        "                sentiment = \"Sad\"\n",
        "            else:\n",
        "                sentiment = \"Neutral\"\n",
        "        else:\n",
        "            sentiment = \"Neutral\"\n",
        "\n",
        "        sentiments.append(sentiment)\n",
        "        cv2.rectangle(annotated_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(annotated_img, sentiment, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "    print(f\"Image: {image_path}\")\n",
        "    for idx, sentiment in enumerate(sentiments):\n",
        "        print(f\"  Person {idx+1}: {sentiment}\")\n",
        "    if sentiments:\n",
        "        sentiment_counts = {'Happy': 0, 'Sad': 0, 'Neutral': 0}\n",
        "        for sentiment in sentiments:\n",
        "            sentiment_counts[sentiment] += 1\n",
        "        overall_sentiment = max(sentiment_counts, key=sentiment_counts.get)\n",
        "        print(f\"Overall sentiment: {overall_sentiment}\\n\")\n",
        "    else:\n",
        "        print(\"No faces detected.\\n\")\n",
        "\n",
        "    success = cv2.imwrite(output_image_path, annotated_img)\n",
        "    if success:\n",
        "        print(f\"Annotated image saved to {output_image_path}\")\n",
        "    else:\n",
        "        print(f\"Error saving annotated image to {output_image_path}\")\n",
        "\n",
        "    #cv2.imshow('Annotated Image', annotated_img)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suHhKpCRyV03"
      },
      "source": [
        "# Task 3: Gender Identification from Facial Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1Fkpdu09yV03",
        "outputId": "24983b89-8614-4e6f-c176-c7bcfdc2400d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image: /content/img6.jpg\n",
            "Predicted Gender: Female\n",
            "Reason: Narrower jawline relative to eye distance suggests female.\n",
            "Geometric Features: {'eye_distance': 21.0, 'jaw_width': 33.0}\n",
            "Image: /content/img6.jpg\n",
            "Predicted Gender: Female\n",
            "Reason: Narrower jawline relative to eye distance suggests female.\n",
            "Geometric Features: {'eye_distance': 33.0, 'jaw_width': 49.0}\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def detect_faces(img):\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    lower_skin = np.array([0, 30, 60], dtype=np.uint8)\n",
        "    upper_skin = np.array([20, 150, 255], dtype=np.uint8)\n",
        "    skin_mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
        "\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
        "    skin_mask = cv2.erode(skin_mask, kernel, iterations=2)\n",
        "    skin_mask = cv2.dilate(skin_mask, kernel, iterations=2)\n",
        "\n",
        "    contours, _ = cv2.findContours(skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    face_regions = []\n",
        "    for cnt in contours:\n",
        "        area = cv2.contourArea(cnt)\n",
        "        if area > 1000:\n",
        "            x, y, w, h = cv2.boundingRect(cnt)\n",
        "            aspect_ratio = w / float(h)\n",
        "\n",
        "            if 0.75 < aspect_ratio < 1.3:\n",
        "\n",
        "                if w > 50 and h > 50:\n",
        "                    face_regions.append((x, y, w, h))\n",
        "    return face_regions\n",
        "\n",
        "def extract_geometric_features(landmarks):\n",
        "    features = {}\n",
        "    eye_distance = np.linalg.norm(np.array(landmarks['left_eye']) - np.array(landmarks['right_eye']))\n",
        "\n",
        "    jaw_width = np.linalg.norm(np.array(landmarks['jaw_left']) - np.array(landmarks['jaw_right']))\n",
        "    features['eye_distance'] = eye_distance\n",
        "    features['jaw_width'] = jaw_width\n",
        "    return features\n",
        "\n",
        "def detect_landmarks(face_img):\n",
        "\n",
        "    height, width = face_img.shape[:2]\n",
        "    landmarks = {\n",
        "        'left_eye': (int(width * 0.3), int(height * 0.4)),\n",
        "        'right_eye': (int(width * 0.7), int(height * 0.4)),\n",
        "        'nose_tip': (int(width * 0.5), int(height * 0.6)),\n",
        "        'jaw_left': (int(width * 0.2), int(height * 0.9)),\n",
        "        'jaw_right': (int(width * 0.8), int(height * 0.9)),\n",
        "        'mouth_left': (int(width * 0.4), int(height * 0.75)),\n",
        "        'mouth_right': (int(width * 0.6), int(height * 0.75)),\n",
        "    }\n",
        "    return landmarks\n",
        "\n",
        "def classify_gender(features):\n",
        "    if features['jaw_width'] / features['eye_distance'] > 1.8:\n",
        "        return 'Male', 'Broad jawline relative to eye distance suggests male.'\n",
        "    else:\n",
        "        return 'Female', 'Narrower jawline relative to eye distance suggests female.'\n",
        "\n",
        "def process_image(image_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error loading image {image_path}\")\n",
        "        return\n",
        "\n",
        "    face_regions = detect_faces(img)\n",
        "    annotated_img = img.copy()\n",
        "    if len(face_regions) == 0:\n",
        "        print(f\"No face detected in {image_path}\")\n",
        "        return\n",
        "\n",
        "    for (x, y, w, h) in face_regions:\n",
        "        face_img = img[y:y+h, x:x+w]\n",
        "        landmarks = detect_landmarks(face_img)\n",
        "        geometric_features = extract_geometric_features(landmarks)\n",
        "        gender, reason = classify_gender(geometric_features)\n",
        "\n",
        "        cv2.rectangle(annotated_img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "        cv2.putText(annotated_img, f\"Predicted: {gender}\", (x, y - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "        for point in landmarks.values():\n",
        "            point_abs = (point[0] + x, point[1] + y)\n",
        "            cv2.circle(annotated_img, point_abs, 2, (255, 0, 0), -1)\n",
        "\n",
        "        print(f\"Image: {image_path}\")\n",
        "        print(f\"Predicted Gender: {gender}\")\n",
        "        print(f\"Reason: {reason}\")\n",
        "        print(f\"Geometric Features: {geometric_features}\")\n",
        "\n",
        "\n",
        "        cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "image_path = '/content/img6.jpg'\n",
        "process_image(image_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}